{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJTR1QSSWyqv"
   },
   "source": [
    "## Deep Neuroevolution Genetic Algorithm Approach to Super Mario Bros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version is able to randomize the world and stage order of the game or evaluate over all stages sequentially while running each agent for multiple episodes each generation to specifically evolve for a better general Neural Mario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean check\n",
    "COLAB = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OHsgaN9asAZ"
   },
   "source": [
    "## Google Colab specific code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the realtime updating of evaluation statistics will not be shown in Google Colab as it does not properly support the carriage return used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9yBB58IKXG9T",
    "outputId": "7e5d0757-8adc-479e-e40c-d4413af87673"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    !pip install gym-super-mario-bros\n",
    "    !mkdir Output\n",
    "    !mkdir .video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-g30xp8caS5U",
    "outputId": "21b78f3c-a75e-4860-956b-1decf3ebe580"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # Connect Google Drive\n",
    "    from google.colab import drive\n",
    "    #drive.mount('/content/gdrive')\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    print('Google Drive connected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WWXRgG2Yod2M"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # Copy over the fittest NeuralMario\n",
    "    !cp \"gdrive/MyDrive/SMB Genetic General/Output/fittestMario.pkl\" -r \"Output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fiRLsFmoeKVu"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # Create a virtual display for video rendering on the headless server.\n",
    "    !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWTdo-hxalUH"
   },
   "source": [
    "## Regular Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Km37L4U9WyrE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import Monitor\n",
    "action_count = 7 # SIMPLE_MOVEMENT\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlgjaNZ7WyrI"
   },
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9OSSOe4OWyrM"
   },
   "outputs": [],
   "source": [
    "# Model Definitions\n",
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)\n",
    "\n",
    "class NeuralMario(nn.Module):\n",
    "        def __init__(self, action_count):\n",
    "            super().__init__()\n",
    "            self.cuda()\n",
    "            self.fc = nn.Sequential(\n",
    "                        nn.Conv2d(4, 8, 3, bias=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Conv2d(8, 6, 3, bias=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        Flatten(),\n",
    "                        nn.Linear(4704, 32, bias=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Linear(32, action_count, bias=True),\n",
    "                        nn.Softmax(dim=1)\n",
    "                        )\n",
    "\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.fc(inputs)\n",
    "            return x\n",
    "\n",
    "def init_weights(m):\n",
    "\n",
    "        # nn.Conv2d weights are of shape [16, 1, 3, 3] i.e. # number of filters, 1, stride, stride\n",
    "        # nn.Conv2d bias is of shape [16] i.e. # number of filters\n",
    "\n",
    "        # nn.Linear weights are of shape [32, 24336] i.e. # number of input features, number of output features\n",
    "        # nn.Linear bias is of shape [32] i.e. # number of output features\n",
    "\n",
    "        if ((type(m) == nn.Linear) | (type(m) == nn.Conv2d)):\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.00)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB1zK08TWyrO"
   },
   "source": [
    "### Environment Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Rb7CO1GFWyrR"
   },
   "outputs": [],
   "source": [
    "# Environment Handling\n",
    "def convert_image(input_image):\n",
    "    image = cv2.cvtColor(input_image, cv2.COLOR_RGB2GRAY)\n",
    "    return cv2.resize(image, (32,32))\n",
    "\n",
    "def run_agent(agent, envs, agent_num, num_agents, rendering=False, print_reward=False, episodes=2):\n",
    "    \n",
    "    global_reward = 0\n",
    "    \n",
    "    for i, env in enumerate(envs):\n",
    "        episode_reward = 0\n",
    "        \n",
    "        agent.eval()\n",
    "        \n",
    "        state = env.reset()\n",
    "        if rendering:\n",
    "            env.render()\n",
    "\n",
    "        #Conv2d without flatten()\n",
    "        state = convert_image(state)#.flatten()\n",
    "        state_list = [state, state, state, state]\n",
    "        position = -1\n",
    "\n",
    "        s=0\n",
    "        \n",
    "        while True:\n",
    "            #Conv2d input\n",
    "            input = torch.from_numpy(np.array(state_list)).type('torch.FloatTensor')\\\n",
    "                .unsqueeze(0)\n",
    "\n",
    "            #Linear input\n",
    "            #input = torch.tensor(state_list).type(\"torch.FloatTensor\").view(1,-1)\n",
    "\n",
    "            output_probabilities = agent(input).detach().numpy()[0]\n",
    "            action = np.random.choice(range(action_count), 1, \\\n",
    "                p=output_probabilities).item()\n",
    "            try:\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "            except:\n",
    "                break\n",
    "            episode_reward += reward\n",
    "\n",
    "            s=s+1\n",
    "            if rendering:\n",
    "                env.render()\n",
    "\n",
    "            state_list.pop()\n",
    "            #Conv2d without flatten()\n",
    "            state_list.append(convert_image(new_state))#.flatten())\n",
    "\n",
    "            # if mario gets stuck, it gets punished and the loop gets broken\n",
    "            if position == info[\"x_pos\"]:\n",
    "                stuck += 1\n",
    "                if stuck == 100:\n",
    "                    episode_reward -= 100\n",
    "                    break\n",
    "            else:\n",
    "                stuck = 0\n",
    "\n",
    "            position = info[\"x_pos\"]\n",
    "            #env.render()\n",
    "            #Mario died\n",
    "            if info[\"life\"] < 2:\n",
    "                break\n",
    "                \n",
    "        env.reset()\n",
    "        \n",
    "        print(\"Agent: \" + str(agent_num+1) + \"/\" + str(num_agents) +\\\n",
    "              \"\\tEpisode: \" + str(i+1) + \"/\" + str(episodes) + \"\\tReward: \"\\\n",
    "              + str(episode_reward) + \"\\t\", end = '\\r')\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        global_reward += episode_reward\n",
    "    \n",
    "    mean_reward = global_reward / episodes\n",
    "    \n",
    "    if print_reward:\n",
    "        print(\"Total:\" + str(global_reward) + \"\\tMean: \" + str(mean_reward))\n",
    "\n",
    "    return mean_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents_n_times(agents, envs, runs, random=True):\n",
    "    avg_score = []\n",
    "    num_agents = len(agents)\n",
    "        \n",
    "    for i, agent in enumerate(agents):\n",
    "        avg_score.append(run_agent(agent, envs, episodes=runs, agent_num=i, num_agents=num_agents))\n",
    "        \n",
    "    return avg_score\n",
    "\n",
    "\n",
    "def return_random_agents(num_agents):\n",
    "    agents = []\n",
    "    for _ in range(num_agents):\n",
    "\n",
    "        agent = NeuralMario(action_count)\n",
    "\n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        init_weights(agent)\n",
    "        agents.append(agent)\n",
    "\n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, rendering=True, monitoring=True, print_reward=True, world=1, stage=1):\n",
    "\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-\" + str(world) + \"-\" + str(stage) + \"-v0\")\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "    if monitoring:\n",
    "        env = Monitor(env, './video', force=True)\n",
    "\n",
    "    env.seed(42)\n",
    "\n",
    "    agent.eval()\n",
    "        \n",
    "    state = env.reset()\n",
    "    if rendering:\n",
    "        env.render()\n",
    "\n",
    "    #Conv2d without flatten()\n",
    "    state = convert_image(state)#.flatten()\n",
    "    state_list = [state, state, state, state]\n",
    "    position = -1\n",
    "    \n",
    "    global_reward = 0\n",
    "    s=0\n",
    "        \n",
    "    for _ in range(30000):\n",
    "        #Conv2d input\n",
    "        input = torch.from_numpy(np.array(state_list)).type('torch.FloatTensor')\\\n",
    "                .unsqueeze(0)\n",
    "\n",
    "        #Linear input\n",
    "        #input = torch.tensor(state_list).type(\"torch.FloatTensor\").view(1,-1)\n",
    "\n",
    "        output_probabilities = agent(input).detach().numpy()[0]\n",
    "        action = np.random.choice(range(action_count), 1, \\\n",
    "            p=output_probabilities).item()\n",
    "        try:\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "        except:\n",
    "            break\n",
    "        global_reward += reward\n",
    "\n",
    "        s=s+1\n",
    "        if rendering:\n",
    "            env.render()\n",
    "\n",
    "        state_list.pop()\n",
    "        #Conv2d without flatten()\n",
    "        state_list.append(convert_image(new_state))#.flatten())\n",
    "\n",
    "        # if mario gets stuck, it gets punished and the loop gets broken\n",
    "        if position == info[\"x_pos\"]:\n",
    "            stuck += 1\n",
    "            if stuck == 100:\n",
    "                global_reward -= 100\n",
    "                break\n",
    "        else:\n",
    "            stuck = 0\n",
    "\n",
    "        position = info[\"x_pos\"]\n",
    "        #env.render()\n",
    "        #Mario died\n",
    "        if info[\"life\"] < 2:\n",
    "            break\n",
    "\n",
    "    return global_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cIOYCiHdWyrU"
   },
   "outputs": [],
   "source": [
    "def return_children(agents, sorted_parent_indexes, elite_count, mutation_power, mutation_chance):\n",
    "    \"\"\" Returning [N-elite_count] mutated agents from sorted_parent_indexes and\n",
    "        keeping the best [elite_count] agents unchanged\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Mutating \" + str(elite_count) + \" elite agent(s) into the next generation.\", end='\\r')\n",
    "\n",
    "    children_agents = []\n",
    "\n",
    "    for i in range(len(agents)-elite_count):\n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "        children_agents.append(mutate(agents[selected_agent_index], mutation_power, mutation_chance))\n",
    "\n",
    "    elite_children = []\n",
    "    for i in range(elite_count):\n",
    "        elite_children.append(agents[sorted_parent_indexes[i]])\n",
    "\n",
    "    children_agents.extend(elite_children)\n",
    "\n",
    "    return children_agents\n",
    "\n",
    "def return_hot_start(elite_agent, num_agents, mutation_power, mutation_chance):\n",
    "    \"\"\" Mutate from a single saved elite agent, or fittestMario, in order to\n",
    "        start evolving a generation immediately without waiting for a single\n",
    "        generation of elite copies to process.\n",
    "        \n",
    "        Only recommended when only a single elite agent is preserved across\n",
    "        generations.\n",
    "    \"\"\"\n",
    "    \n",
    "    children_agents = []\n",
    "    \n",
    "    for i in range(num_agents-1):\n",
    "        children_agents.append(mutate(elite_agent, mutation_power, mutation_chance))\n",
    "\n",
    "    children_agents.extend([elite_agent])\n",
    "    \n",
    "    return children_agents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(agent, power=0.2, chance=0.02):\n",
    "    ''' Simple method to add gaussian noise to the agents '''\n",
    "\n",
    "    child_agent = copy.deepcopy(agent)\n",
    "\n",
    "    mutation_power = power #hyper-parameter, 0.2 set from https://arxiv.org/pdf/1712.06567.pdf\n",
    "    \n",
    "    # Current checks for mutation: 33189\n",
    "    mutation_chance = chance\n",
    "    \n",
    "    for param in child_agent.parameters():\n",
    "        \n",
    "        if(len(param.shape)==4): #weights of Conv2D\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    for i2 in range(param.shape[2]):\n",
    "                        if np.random.random(1)[0] <= mutation_chance:\n",
    "                            for i3 in range(param.shape[3]):\n",
    "                                param[i0][i1][i2][i3]+= mutation_power * np.random.randn()\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "        elif(len(param.shape)==2): #weights of linear layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    if np.random.random(1)[0] <= mutation_chance:\n",
    "                        param[i0][i1]+= mutation_power * np.random.randn()\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        elif(len(param.shape)==1): #biases of linear layer or conv layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                if np.random.random(1)[0] <= mutation_chance:\n",
    "                    param[i0]+=mutation_power * np.random.randn()\n",
    "                else:\n",
    "                    pass\n",
    "    \n",
    "    \n",
    "    return child_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(elite_agent, mutation_power, mutation_chance, episodes, agents=10, elites=1, show_top=2, generations=100, hot_start=False, random=True):\n",
    "    torch.set_grad_enabled(False)\n",
    "    num_agents = agents\n",
    "    elite_count = elites\n",
    "    top_limit_count = show_top\n",
    "    n_episodes = episodes\n",
    "\n",
    "    generation_count = generations\n",
    "\n",
    "    if (elite_agent != None) and (not hot_start):\n",
    "        print(\"Loaded \" + str(num_agents) + \" elite copies.\")\n",
    "        agents = [elite_agent]*num_agents\n",
    "    elif (elite_agent != None) and hot_start:\n",
    "        print(\"Hot start enabled. Mutating loaded elite \" + str(num_agents-1) + \" times.\")\n",
    "        agents = return_hot_start(elite_agent, num_agents, mutation_power, mutation_chance)\n",
    "    elif (elite_agent == None) and hot_start:\n",
    "        print(\"Hot start enabled without required elite agent.\")\n",
    "        print(\"Creating \" + str(num_agents) + \" random agents.\")\n",
    "        agents = return_random_agents(num_agents)\n",
    "    else:\n",
    "        print(\"Creating \" + str(num_agents) + \" random agents.\")\n",
    "        agents = return_random_agents(num_agents)\n",
    "        \n",
    "        \n",
    "    # Create all environments to use throughout the generation loop\n",
    "    envs = []\n",
    "    \n",
    "    worlds = []\n",
    "    stages = []\n",
    "    \n",
    "    if not random:\n",
    "        '''\n",
    "        for i in range(8):\n",
    "            for j in range(4):\n",
    "                worlds.append(i+1)\n",
    "                stages.append(j+1)\n",
    "        '''\n",
    "        \n",
    "        #worlds = [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8]\n",
    "        #stages = [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]\n",
    "        worlds = [1, 1, 1, 1]\n",
    "        stages = [1, 2, 3, 4]\n",
    "        \n",
    "                \n",
    "        for i in range(len(worlds)):\n",
    "            y = str(worlds[i])\n",
    "            z = str(stages[i])\n",
    "            env = gym_super_mario_bros.make(\"SuperMarioBros-\" + y + \"-\" + z + \"-v0\")\n",
    "            env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "            env.seed(42)\n",
    "\n",
    "            envs.append(env)\n",
    "\n",
    "    \n",
    "    for generation in range(generation_count):\n",
    "        print(\"############################### Generation {} ###############################\".format(generation+1))\n",
    "        \n",
    "        if random:\n",
    "            envs = []\n",
    "            for i in range(n_episodes):\n",
    "                worlds.append(np.random.randint(1, 9))\n",
    "                stages.append(np.random.randint(1, 5))\n",
    "                \n",
    "            for i in range(len(worlds)):\n",
    "                y = str(worlds[i])\n",
    "                z = str(stages[i])\n",
    "                env = gym_super_mario_bros.make(\"SuperMarioBros-\" + y + \"-\" + z + \"-v0\")\n",
    "                env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "                env.seed(42)\n",
    "\n",
    "                envs.append(env)\n",
    "        \n",
    "        rewards = run_agents_n_times(agents, envs, n_episodes, random)\n",
    "\n",
    "        sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit_count]\n",
    "        \n",
    "        top_rewards = []\n",
    "        for best_parent in sorted_parent_indexes:\n",
    "            top_rewards.append(rewards[best_parent])\n",
    "\n",
    "        print(\"Mean reward: {}\\t\\tMean of top 5: {}\".format(\\\n",
    "            int(np.mean(rewards)), int(np.mean(top_rewards[:5]))))\n",
    "        print(\"Top agents: {}\\tReward: {}\".format(sorted_parent_indexes, \\\n",
    "            top_rewards))\n",
    "        print(\"#############################################################################\\n\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        \n",
    "        with open(\"Output/fittestMario.pkl\", 'wb') as output:\n",
    "            pickle.dump(agents[sorted_parent_indexes[0]], output, \\\n",
    "                pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        children_agents = return_children(agents, sorted_parent_indexes, elite_count, mutation_power, mutation_chance)\n",
    "\n",
    "        agents = children_agents\n",
    "        \n",
    "        if COLAB:\n",
    "            !cp \"Output\" -r \"gdrive/MyDrive/SMB Genetic General/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAxnKa02WyrW"
   },
   "source": [
    "### Model Loading and Training\n",
    "The training function is defined here.\n",
    "Control Arguments:\n",
    "* <b>training:</b> If True, the genetic algorithm evolves the neural networks, If False, the best agent is loaded and plays a game, with the gym monitor recording the video.\n",
    "\n",
    "* <b>load_elite:</b> If True, the previous elite agent is loaded from file. If False, all previous trainings are ignored and overwritten.\n",
    "\n",
    "* <b>agents:</b> The number of agents in the population.\n",
    "\n",
    "* <b>elites:</b> The number of elite agents to carry over across generations, and to mutate from.\n",
    "\n",
    "* <b>show_top:</b> The number of top agent rewards scores to display in the evolution output.\n",
    "\n",
    "* <b>generations:</b> The number of generations to create and test.\n",
    "\n",
    "* <b>mutation_power:</b> The Gaussian noise multiplier for the mutation function to use.\n",
    "\n",
    "* <b>mutation_chance:</b> The percentage chance for a genotype to undergo mutation within a generation creation.\n",
    "\n",
    "* <b>episodes:</b> The number of game runs to use when evaluating the performance of each agent within a generation.\n",
    "\n",
    "* <b>hot_start: Not recommended if implementing multiple-agent elitism.</b> If True, create the first generation via mutations of the single loaded elite agent from a previous session. If False, make copies of the loaded elite agent if available or create a generation of completely random agents.\n",
    "\n",
    "* <b>random:</b> If True, randomly sample stages across all worlds for the evaluation of each agent each generation. If False, evaluate via every stage in the game sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kwizQF_2WyrZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Training\n",
    "def train(training=True, load_elite=True, agents=10, elites=1, show_top=2, generations=100, mutation_power=0.2, mutation_chance=0.02, episodes=2, hot_start=False, random=True):\n",
    "    if training:\n",
    "        \n",
    "        elite_agent = None\n",
    "        if load_elite:\n",
    "            with open(\"Output/fittestMario.pkl\", 'rb') as input:\n",
    "                elite_agent = pickle.load(input)\n",
    "        \n",
    "        # If not randomly sampling, all 32 stages are sequentially tested.\n",
    "        if not random:\n",
    "            print()\n",
    "            episodes = 4\n",
    "            \n",
    "        main(elite_agent, mutation_power, mutation_chance, episodes, agents, elites, show_top, generations, hot_start, random)\n",
    "\n",
    "    else:\n",
    "        with open(\"Output/fittestMario.pkl\", 'rb') as input:\n",
    "            fittest_mario = pickle.load(input)\n",
    "            test_agent(fittest_mario, True, True, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dIuiwpiWyra",
    "outputId": "bac618f1-935c-4fe6-9213-a6946f58c663",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hot start enabled. Mutating loaded elite 499 times.\n",
      "############################### Generation 1 ###############################\n",
      "Mean reward: 518\t\tMean of top 5: 1022135\t\n",
      "Top agents: [499 218  42 445  51]\tReward: [1239.25, 1121.75, 918.75, 916.5, 914.0]\n",
      "#############################################################################\n",
      "\n",
      "############################### Generation 2 ###############################\n",
      "Mean reward: 559\t\tMean of top 5: 1027135\t\n",
      "Top agents: [499 219 154 187  74]\tReward: [1239.25, 1012.75, 990.75, 948.0, 944.25]\n",
      "#############################################################################\n",
      "\n",
      "############################### Generation 3 ###############################\n",
      "Mean reward: 540\t\tMean of top 5: 1065135\t\n",
      "Top agents: [499  82 454   3 140]\tReward: [1239.25, 1098.0, 1018.75, 985.25, 985.0]\n",
      "#############################################################################\n",
      "\n",
      "############################### Generation 4 ###############################\n",
      "Mean reward: 546\t\tMean of top 5: 1022135\t\n",
      "Top agents: [499 325 168 223 191]\tReward: [1239.25, 1024.5, 981.75, 936.5, 932.0]\n",
      "#############################################################################\n",
      "\n",
      "############################### Generation 5 ###############################\n",
      "Mean reward: 570\t\tMean of top 5: 1015135\t\n",
      "Top agents: [499  86 461 263  88]\tReward: [1239.25, 988.75, 955.5, 948.0, 947.25]\n",
      "#############################################################################\n",
      "\n",
      "############################### Generation 6 ###############################\n",
      "Mean reward: 566\t\tMean of top 5: 1031135\t\n",
      "Top agents: [499  76 158 249 300]\tReward: [1239.25, 1027.5, 983.0, 965.5, 941.75]\n",
      "#############################################################################\n",
      "\n",
      "############################### Generation 7 ###############################\n",
      "Mean reward: 568\t\tMean of top 5: 1092135\t\n",
      "Top agents: [499  39  32 465  99]\tReward: [1239.25, 1235.5, 1005.75, 1003.0, 980.75]\n",
      "#############################################################################\n",
      "\n",
      "############################### Generation 8 ###############################\n",
      "Mean reward: 592\t\tMean of top 5: 1077135\t\n",
      "Top agents: [499 464 436 283 467]\tReward: [1239.25, 1095.25, 1087.5, 987.0, 977.0]\n",
      "#############################################################################\n",
      "\n",
      "############################### Generation 9 ###############################\n",
      "Mean reward: 575\t\tMean of top 5: 1064135\t\n",
      "Top agents: [499 115 349 393  21]\tReward: [1239.25, 1100.75, 1002.5, 997.25, 983.5]\n",
      "#############################################################################\n",
      "\n",
      "Mutating 1 elite agent(s) into the next generation.\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7beedcf2ad33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train, load, agents, elite, display top, generations, mutation_power, mutation_chance, episodes per generation, hot_start, randomize stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-4e1e6005b362>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(training, load_elite, agents, elites, show_top, generations, mutation_power, mutation_chance, episodes, hot_start, random)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mepisodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melite_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutation_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutation_chance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melites\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_top\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhot_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-9cfa06638283>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(elite_agent, mutation_power, mutation_chance, episodes, agents, elites, show_top, generations, hot_start, random)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msorted_parent_indexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m                 \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mchildren_agents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_children\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_parent_indexes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melite_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutation_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutation_chance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0magents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchildren_agents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-7219e7527715>\u001b[0m in \u001b[0;36mreturn_children\u001b[1;34m(agents, sorted_parent_indexes, elite_count, mutation_power, mutation_chance)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0melite_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mselected_agent_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted_parent_indexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted_parent_indexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mchildren_agents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmutate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselected_agent_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutation_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutation_chance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0melite_children\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-1db07e94f04c>\u001b[0m in \u001b[0;36mmutate\u001b[1;34m(agent, power, chance)\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mmutation_chance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                         \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m \u001b[0mmutation_power\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train, load, agents, elite, display top, generations, mutation_power, mutation_chance, episodes per generation, hot_start, randomize stages\n",
    "train(True, True, 500, 1, 5, 75, 0.2, 0.025, 4, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUdfD-lMWyrf"
   },
   "source": [
    "This will now run the best neural network one more time, wrapped in the monitor environment to record video of play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gSJQTjO6Wyrj"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    try:\n",
    "        display.start()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "with open(\"Output/fittestMario.pkl\", 'rb') as input:\n",
    "    fittest_mario = pickle.load(input)\n",
    "    test_agent(fittest_mario, True, True, True, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkyj0n_bbQnv"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    !cp \"video\" -r \"gdrive/MyDrive/SMB Genetic General/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Neuroevolution SMB PreGPU Backup.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mario_dqn",
   "language": "python",
   "name": "mario_dqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
