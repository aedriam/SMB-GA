{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJTR1QSSWyqv"
   },
   "source": [
    "## Deep Neuroevolution Genetic Algorithm Approach to Super Mario Bros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version is able to randomize the world and stage order of the game or evaluate over all stages sequentially while running each agent for multiple episodes each generation to specifically evolve for a better general Neural Mario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean check\n",
    "COLAB = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OHsgaN9asAZ"
   },
   "source": [
    "## Google Colab specific code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the realtime updating of evaluation statistics will not be shown in Google Colab as it does not properly support the carriage return used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9yBB58IKXG9T",
    "outputId": "7e5d0757-8adc-479e-e40c-d4413af87673"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    !pip install gym-super-mario-bros\n",
    "    !mkdir Output\n",
    "    !mkdir .video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-g30xp8caS5U",
    "outputId": "21b78f3c-a75e-4860-956b-1decf3ebe580"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # Connect Google Drive\n",
    "    from google.colab import drive\n",
    "    #drive.mount('/content/gdrive')\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    print('Google Drive connected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWXRgG2Yod2M"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # Copy over the fittest NeuralMario\n",
    "    !cp \"gdrive/MyDrive/SMB Genetic General/Output/fittestMario.pkl\" -r \"Output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiRLsFmoeKVu"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    # Create a virtual display for video rendering on the headless server.\n",
    "    !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "    !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWTdo-hxalUH"
   },
   "source": [
    "## Regular Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Km37L4U9WyrE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from gym.wrappers import Monitor\n",
    "action_count = 7 # SIMPLE_MOVEMENT\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlgjaNZ7WyrI"
   },
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OSSOe4OWyrM"
   },
   "outputs": [],
   "source": [
    "# Model Definitions\n",
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)\n",
    "\n",
    "class NeuralMario(nn.Module):\n",
    "        def __init__(self, action_count):\n",
    "            super().__init__()\n",
    "            self.cuda()\n",
    "            self.fc = nn.Sequential(\n",
    "                        nn.Conv2d(4, 8, 3, bias=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Conv2d(8, 6, 3, bias=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        Flatten(),\n",
    "                        nn.Linear(4704, 32, bias=True),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Linear(32, action_count, bias=True),\n",
    "                        nn.Softmax(dim=1)\n",
    "                        )\n",
    "\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.fc(inputs)\n",
    "            return x\n",
    "\n",
    "def init_weights(m):\n",
    "\n",
    "        # nn.Conv2d weights are of shape [16, 1, 3, 3] i.e. # number of filters, 1, stride, stride\n",
    "        # nn.Conv2d bias is of shape [16] i.e. # number of filters\n",
    "\n",
    "        # nn.Linear weights are of shape [32, 24336] i.e. # number of input features, number of output features\n",
    "        # nn.Linear bias is of shape [32] i.e. # number of output features\n",
    "\n",
    "        if ((type(m) == nn.Linear) | (type(m) == nn.Conv2d)):\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.00)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB1zK08TWyrO"
   },
   "source": [
    "### Environment Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rb7CO1GFWyrR"
   },
   "outputs": [],
   "source": [
    "# Environment Handling\n",
    "def convert_image(input_image):\n",
    "    image = cv2.cvtColor(input_image, cv2.COLOR_RGB2GRAY)\n",
    "    return cv2.resize(image, (32,32))\n",
    "\n",
    "def run_agent(agent, envs, agent_num, num_agents, rendering=False, print_reward=False, episodes=2):\n",
    "    \n",
    "    global_reward = 0\n",
    "    \n",
    "    for i, env in enumerate(envs):\n",
    "        episode_reward = 0\n",
    "        \n",
    "        agent.eval()\n",
    "        \n",
    "        state = env.reset()\n",
    "        if rendering:\n",
    "            env.render()\n",
    "\n",
    "        #Conv2d without flatten()\n",
    "        state = convert_image(state)#.flatten()\n",
    "        state_list = [state, state, state, state]\n",
    "        position = -1\n",
    "\n",
    "        s=0\n",
    "        \n",
    "        while True:\n",
    "            #Conv2d input\n",
    "            input = torch.from_numpy(np.array(state_list)).type('torch.FloatTensor')\\\n",
    "                .unsqueeze(0)\n",
    "\n",
    "            #Linear input\n",
    "            #input = torch.tensor(state_list).type(\"torch.FloatTensor\").view(1,-1)\n",
    "\n",
    "            output_probabilities = agent(input).detach().numpy()[0]\n",
    "            action = np.random.choice(range(action_count), 1, \\\n",
    "                p=output_probabilities).item()\n",
    "            try:\n",
    "                new_state, reward, done, info = env.step(action)\n",
    "            except:\n",
    "                break\n",
    "            episode_reward += reward\n",
    "\n",
    "            s=s+1\n",
    "            if rendering:\n",
    "                env.render()\n",
    "\n",
    "            state_list.pop()\n",
    "            #Conv2d without flatten()\n",
    "            state_list.append(convert_image(new_state))#.flatten())\n",
    "\n",
    "            # if mario gets stuck, it gets punished and the loop gets broken\n",
    "            if position == info[\"x_pos\"]:\n",
    "                stuck += 1\n",
    "                if stuck == 100:\n",
    "                    episode_reward -= 100\n",
    "                    break\n",
    "            else:\n",
    "                stuck = 0\n",
    "\n",
    "            position = info[\"x_pos\"]\n",
    "            #env.render()\n",
    "            #Mario died\n",
    "            if info[\"life\"] < 2:\n",
    "                break\n",
    "                \n",
    "        env.reset()\n",
    "        \n",
    "        print(\"Agent: \" + str(agent_num+1) + \"/\" + str(num_agents) +\\\n",
    "              \"\\tEpisode: \" + str(i+1) + \"/\" + str(episodes) + \"\\tReward: \"\\\n",
    "              + str(episode_reward) + \"\\t\", end = '\\r')\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        global_reward += episode_reward\n",
    "    \n",
    "    mean_reward = global_reward / episodes\n",
    "    \n",
    "    if print_reward:\n",
    "        print(\"Total:\" + str(global_reward) + \"\\tMean: \" + str(mean_reward))\n",
    "\n",
    "    return mean_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents_n_times(agents, envs, runs, random=True):\n",
    "    avg_score = []\n",
    "    num_agents = len(agents)\n",
    "        \n",
    "    for i, agent in enumerate(agents):\n",
    "        avg_score.append(run_agent(agent, envs, episodes=runs, agent_num=i, num_agents=num_agents))\n",
    "        \n",
    "    return avg_score\n",
    "\n",
    "\n",
    "def return_random_agents(num_agents):\n",
    "    agents = []\n",
    "    for _ in range(num_agents):\n",
    "\n",
    "        agent = NeuralMario(action_count)\n",
    "\n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        init_weights(agent)\n",
    "        agents.append(agent)\n",
    "\n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, rendering=True, monitoring=True, print_reward=True, world=1, stage=1):\n",
    "\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-\" + str(world) + \"-\" + str(stage) + \"-v0\")\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "    if monitoring:\n",
    "        env = Monitor(env, './video', force=True)\n",
    "\n",
    "    env.seed(42)\n",
    "\n",
    "    agent.eval()\n",
    "        \n",
    "    state = env.reset()\n",
    "    if rendering:\n",
    "        env.render()\n",
    "\n",
    "    #Conv2d without flatten()\n",
    "    state = convert_image(state)#.flatten()\n",
    "    state_list = [state, state, state, state]\n",
    "    position = -1\n",
    "    \n",
    "    global_reward = 0\n",
    "    s=0\n",
    "        \n",
    "    for _ in range(30000):\n",
    "        #Conv2d input\n",
    "        input = torch.from_numpy(np.array(state_list)).type('torch.FloatTensor')\\\n",
    "                .unsqueeze(0)\n",
    "\n",
    "        #Linear input\n",
    "        #input = torch.tensor(state_list).type(\"torch.FloatTensor\").view(1,-1)\n",
    "\n",
    "        output_probabilities = agent(input).detach().numpy()[0]\n",
    "        action = np.random.choice(range(action_count), 1, \\\n",
    "            p=output_probabilities).item()\n",
    "        try:\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "        except:\n",
    "            break\n",
    "        global_reward += reward\n",
    "\n",
    "        s=s+1\n",
    "        if rendering:\n",
    "            env.render()\n",
    "\n",
    "        state_list.pop()\n",
    "        #Conv2d without flatten()\n",
    "        state_list.append(convert_image(new_state))#.flatten())\n",
    "\n",
    "        # if mario gets stuck, it gets punished and the loop gets broken\n",
    "        if position == info[\"x_pos\"]:\n",
    "            stuck += 1\n",
    "            if stuck == 100:\n",
    "                global_reward -= 100\n",
    "                break\n",
    "        else:\n",
    "            stuck = 0\n",
    "\n",
    "        position = info[\"x_pos\"]\n",
    "        #env.render()\n",
    "        #Mario died\n",
    "        if info[\"life\"] < 2:\n",
    "            break\n",
    "\n",
    "    return global_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIOYCiHdWyrU"
   },
   "outputs": [],
   "source": [
    "def return_children(agents, sorted_parent_indexes, elite_count, mutation_power, mutation_chance):\n",
    "    \"\"\" Returning [N-elite_count] mutated agents from sorted_parent_indexes and\n",
    "        keeping the best [elite_count] agents unchanged\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Mutating \" + str(elite_count) + \" elite agent(s) into the next generation.\", end='\\r')\n",
    "\n",
    "    children_agents = []\n",
    "\n",
    "    for i in range(len(agents)-elite_count):\n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "        children_agents.append(mutate(agents[selected_agent_index], mutation_power, mutation_chance))\n",
    "\n",
    "    elite_children = []\n",
    "    for i in range(elite_count):\n",
    "        elite_children.append(agents[sorted_parent_indexes[i]])\n",
    "\n",
    "    children_agents.extend(elite_children)\n",
    "\n",
    "    return children_agents\n",
    "\n",
    "def return_hot_start(elite_agent, num_agents, mutation_power, mutation_chance):\n",
    "    \"\"\" Mutate from a single saved elite agent, or fittestMario, in order to\n",
    "        start evolving a generation immediately without waiting for a single\n",
    "        generation of elite copies to process.\n",
    "        \n",
    "        Only recommended when only a single elite agent is preserved across\n",
    "        generations.\n",
    "    \"\"\"\n",
    "    \n",
    "    children_agents = []\n",
    "    \n",
    "    for i in range(num_agents-1):\n",
    "        children_agents.append(mutate(elite_agent, mutation_power, mutation_chance))\n",
    "\n",
    "    children_agents.extend([elite_agent])\n",
    "    \n",
    "    return children_agents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(agent, power=0.2, chance=0.02):\n",
    "    ''' Simple method to add gaussian noise to the agents '''\n",
    "\n",
    "    child_agent = copy.deepcopy(agent)\n",
    "\n",
    "    mutation_power = power #hyper-parameter, 0.2 set from https://arxiv.org/pdf/1712.06567.pdf\n",
    "    \n",
    "    # Current checks for mutation: 33189\n",
    "    mutation_chance = chance\n",
    "    \n",
    "    for param in child_agent.parameters():\n",
    "        \n",
    "        if(len(param.shape)==4): #weights of Conv2D\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    for i2 in range(param.shape[2]):\n",
    "                        if np.random.random(1)[0] <= mutation_chance:\n",
    "                            for i3 in range(param.shape[3]):\n",
    "                                param[i0][i1][i2][i3]+= mutation_power * np.random.randn()\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "        elif(len(param.shape)==2): #weights of linear layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    if np.random.random(1)[0] <= mutation_chance:\n",
    "                        param[i0][i1]+= mutation_power * np.random.randn()\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        elif(len(param.shape)==1): #biases of linear layer or conv layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                if np.random.random(1)[0] <= mutation_chance:\n",
    "                    param[i0]+=mutation_power * np.random.randn()\n",
    "                else:\n",
    "                    pass\n",
    "    \n",
    "    \n",
    "    return child_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(elite_agent, mutation_power, mutation_chance, episodes, agents=10, elites=1, show_top=2, generations=100, hot_start=False, random=True):\n",
    "    torch.set_grad_enabled(False)\n",
    "    num_agents = agents\n",
    "    elite_count = elites\n",
    "    top_limit_count = show_top\n",
    "    n_episodes = episodes\n",
    "\n",
    "    generation_count = generations\n",
    "\n",
    "    if (elite_agent != None) and (not hot_start):\n",
    "        print(\"Loaded \" + str(num_agents) + \" elite copies.\")\n",
    "        agents = [elite_agent]*num_agents\n",
    "    elif (elite_agent != None) and hot_start:\n",
    "        print(\"Hot start enabled. Mutating loaded elite \" + str(num_agents-1) + \" times.\")\n",
    "        agents = return_hot_start(elite_agent, num_agents, mutation_power, mutation_chance)\n",
    "    elif (elite_agent == None) and hot_start:\n",
    "        print(\"Hot start enabled without required elite agent.\")\n",
    "        print(\"Creating \" + str(num_agents) + \" random agents.\")\n",
    "        agents = return_random_agents(num_agents)\n",
    "    else:\n",
    "        print(\"Creating \" + str(num_agents) + \" random agents.\")\n",
    "        agents = return_random_agents(num_agents)\n",
    "        \n",
    "        \n",
    "    # Create all environments to use throughout the generation loop\n",
    "    envs = []\n",
    "    \n",
    "    worlds = []\n",
    "    stages = []\n",
    "    \n",
    "    if not random:\n",
    "        for i in range(8):\n",
    "            for j in range(4):\n",
    "                worlds.append(i+1)\n",
    "                stages.append(j+1)\n",
    "                \n",
    "        for i in range(len(worlds)):\n",
    "            y = str(worlds[i])\n",
    "            z = str(stages[i])\n",
    "            env = gym_super_mario_bros.make(\"SuperMarioBros-\" + y + \"-\" + z + \"-v0\")\n",
    "            env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "            env.seed(42)\n",
    "\n",
    "            envs.append(env)\n",
    "\n",
    "    \n",
    "    for generation in range(generation_count):\n",
    "        print(\"############################### Generation {} ###############################\".format(generation+1))\n",
    "        \n",
    "        if random:\n",
    "            envs = []\n",
    "            for i in range(n_episodes):\n",
    "                worlds.append(np.random.randint(1, 9))\n",
    "                stages.append(np.random.randint(1, 5))\n",
    "                \n",
    "            for i in range(len(worlds)):\n",
    "                y = str(worlds[i])\n",
    "                z = str(stages[i])\n",
    "                env = gym_super_mario_bros.make(\"SuperMarioBros-\" + y + \"-\" + z + \"-v0\")\n",
    "                env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "                env.seed(42)\n",
    "\n",
    "                envs.append(env)\n",
    "        \n",
    "        rewards = run_agents_n_times(agents, envs, n_episodes, random)\n",
    "\n",
    "        sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit_count]\n",
    "        \n",
    "        top_rewards = []\n",
    "        for best_parent in sorted_parent_indexes:\n",
    "            top_rewards.append(rewards[best_parent])\n",
    "\n",
    "        print(\"Mean reward: {}\\t\\tMean of top 5: {}\".format(\\\n",
    "            int(np.mean(rewards)), int(np.mean(top_rewards[:5]))))\n",
    "        print(\"Top agents: {}\\tReward: {}\".format(sorted_parent_indexes, \\\n",
    "            top_rewards))\n",
    "        print(\"#############################################################################\\n\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        \n",
    "        with open(\"Output/fittestMario.pkl\", 'wb') as output:\n",
    "            pickle.dump(agents[sorted_parent_indexes[0]], output, \\\n",
    "                pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        children_agents = return_children(agents, sorted_parent_indexes, elite_count, mutation_power, mutation_chance)\n",
    "\n",
    "        agents = children_agents\n",
    "        \n",
    "        if COLAB:\n",
    "            !cp \"Output\" -r \"gdrive/MyDrive/SMB Genetic General/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAxnKa02WyrW"
   },
   "source": [
    "### Model Loading and Training\n",
    "The training function is defined here.\n",
    "Control Arguments:\n",
    "* <b>training:</b> If True, the genetic algorithm evolves the neural networks, If False, the best agent is loaded and plays a game, with the gym monitor recording the video.\n",
    "\n",
    "* <b>load_elite:</b> If True, the previous elite agent is loaded from file. If False, all previous trainings are ignored and overwritten.\n",
    "\n",
    "* <b>agents:</b> The number of agents in the population.\n",
    "\n",
    "* <b>elites:</b> The number of elite agents to carry over across generations, and to mutate from.\n",
    "\n",
    "* <b>show_top:</b> The number of top agent rewards scores to display in the evolution output.\n",
    "\n",
    "* <b>generations:</b> The number of generations to create and test.\n",
    "\n",
    "* <b>mutation_power:</b> The Gaussian noise multiplier for the mutation function to use.\n",
    "\n",
    "* <b>mutation_chance:</b> The percentage chance for a genotype to undergo mutation within a generation creation.\n",
    "\n",
    "* <b>episodes:</b> The number of game runs to use when evaluating the performance of each agent within a generation.\n",
    "\n",
    "* <b>hot_start: Not recommended if implementing multiple-agent elitism.</b> If True, create the first generation via mutations of the single loaded elite agent from a previous session. If False, make copies of the loaded elite agent if available or create a generation of completely random agents.\n",
    "\n",
    "* <b>random:</b> If True, randomly sample stages across all worlds for the evaluation of each agent each generation. If False, evaluate via every stage in the game sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwizQF_2WyrZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model Training\n",
    "def train(training=True, load_elite=True, agents=10, elites=1, show_top=2, generations=100, mutation_power=0.2, mutation_chance=0.02, episodes=2, hot_start=False, random=True):\n",
    "    if training:\n",
    "        \n",
    "        elite_agent = None\n",
    "        if load_elite:\n",
    "            with open(\"Output/fittestMario.pkl\", 'rb') as input:\n",
    "                elite_agent = pickle.load(input)\n",
    "        \n",
    "        # If not randomly sampling, all 32 stages are sequentially tested.\n",
    "        if not random:\n",
    "            print()\n",
    "            episodes = 32\n",
    "            \n",
    "        main(elite_agent, mutation_power, mutation_chance, episodes, agents, elites, show_top, generations, hot_start, random)\n",
    "\n",
    "    else:\n",
    "        with open(\"Output/fittestMario.pkl\", 'rb') as input:\n",
    "            fittest_mario = pickle.load(input)\n",
    "            test_agent(fittest_mario, True, True, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dIuiwpiWyra",
    "outputId": "bac618f1-935c-4fe6-9213-a6946f58c663",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train, load, agents, elite, display top, generations, mutation_power, mutation_chance, episodes per generation, hot_start, randomize stages\n",
    "train(True, True, 100, 1, 5, 50, 0.2, 0.025, 32, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUdfD-lMWyrf"
   },
   "source": [
    "This will now run the best neural network one more time, wrapped in the monitor environment to record video of play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gSJQTjO6Wyrj"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    try:\n",
    "        display.start()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "with open(\"Output/fittestMario.pkl\", 'rb') as input:\n",
    "    fittest_mario = pickle.load(input)\n",
    "    test_agent(fittest_mario, True, True, True, 8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkyj0n_bbQnv"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    !cp \"video\" -r \"gdrive/MyDrive/SMB Genetic General/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Neuroevolution SMB PreGPU Backup.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mario_dqn",
   "language": "python",
   "name": "mario_dqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
